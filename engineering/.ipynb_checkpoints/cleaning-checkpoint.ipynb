{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong>In This Notebook...</strong></h2><br />\n",
    "This is for data cleaning and engineering for our project.  Much inspiration received from <a href=\"https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering/notebook\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.72 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import warnings\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# declare some strings\n",
    "id_column = 'id'\n",
    "missing_token = ' UNK '\n",
    "\n",
    "# read in our data, parse_dates=['column name'] will read that column as a datetime object, can take a boolean, list of integers / names, list of lists or a dictionary,\n",
    "# does different things depending on which one you use read the docs~\n",
    "train = pd.read_csv('H:/uci_data/donorschoose-application-screening/train/train.csv', parse_dates=['project_submitted_datetime'])\n",
    "test = pd.read_csv('H:/uci_data/donorschoose-application-screening/test/test.csv', parse_dates=['project_submitted_datetime'])\n",
    "hopes = pd.read_csv('H:/uci_data/donorschoose-application-screening/resources/resources.csv').fillna(missing_token)\n",
    "\n",
    "# # lets make a master df of the train and test data to make our lives easier!\n",
    "# df = pd.concat([train,test], axis=0)\n",
    "# no lets not that was awful\n",
    "df = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathy Features\n",
    "+ Min, Max, Mean Price for resources requested\n",
    "+ Min Quantity, Max Quantity, Mean Quantity of resources requested\n",
    "+ Min Total Price, Max Total Price, Mean Total Price of resources requested\n",
    "+ Total Price of items requested by proposal\n",
    "+ Number of Unique Items Requested by proposal\n",
    "+ Quantity of items requested in proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.89 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# A new column for total price\n",
    "hopes['total_price'] = hopes['quantity']*hopes['price']\n",
    "\n",
    "# Make an aggregate df to join to our normal df\n",
    "# the .agg method takes in a function, string, or a dictionary or list of strings or functions.  The dictionary keys will be column names upon which functions should be run\n",
    "# I named it after the horse in Shadow of the Colossus~ the description column is now a count of how many, so it can be renamed to (number of )items\n",
    "agro = {'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}\n",
    "aggregatedf = hopes.groupby('id').agg(agro).rename(columns={'description':'items'})\n",
    "\n",
    "# now lets use that string functionality of .agg to get the min, max, and mean values!\n",
    "for maths in ['min', 'max', 'mean']:\n",
    "    # romanized Japanese horse name from game, and that guy that changes names in ff because why not lets have fun with variable names they're just for here anyway\n",
    "    aguro = {'quantity':maths, 'price':maths, 'total_price':maths}\n",
    "    namingway = {'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}\n",
    "    \n",
    "    # do some aggregation and join it to our previously created df\n",
    "    temporary = hopes.groupby('id').agg(aguro).rename(columns=namingway).fillna(0)\n",
    "    aggregatedf = aggregatedf.join(temporary)\n",
    "# This didn't work whoops # aggregatedf = aggregatedf.join([hopes.groupby('id').agg({'quantity':maths, 'price':maths, 'total_price':maths}).rename(columns={'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}).fillna(0) for maths in ['min', 'max', 'mean']])\n",
    "\n",
    "# and finally give it the original description columns aggregated together with a space in between them\n",
    "aggregatedf = aggregatedf.join(hopes.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n",
    "\n",
    "# Join that together with our everything df and check it out\n",
    "df = df.join(aggregatedf, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great, now lets play with time!\n",
    "+ Year of submission\n",
    "+ Month of submission\n",
    "+ Year Day (1-365) of submission\n",
    "+ Month Day (1-31) of submission\n",
    "+ Week Day (1-7) of submission\n",
    "+ Hour of submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 89.1 ms\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# using datetime to make the above features\n",
    "df['year'] = df['project_submitted_datetime'].dt.year\n",
    "df['month'] = df['project_submitted_datetime'].dt.month\n",
    "df['year_day'] = df['project_submitted_datetime'].dt.dayofyear\n",
    "df['month_day'] = df['project_submitted_datetime'].dt.day\n",
    "df['week_day'] = df['project_submitted_datetime'].dt.weekday\n",
    "df['hour'] = df['project_submitted_datetime'].dt.hour\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text based features\n",
    "+ Length of essays including spaces\n",
    "+ Length of project title\n",
    "+ Word count across essays\n",
    "+ Character count across essays\n",
    "+ Word density / average length of words used\n",
    "+ Punctuation count\n",
    "+ Uppercase count\n",
    "+ Title Word Count (Gotta Have This Case)\n",
    "+ Stopword Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.9 ms\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# fill empty values with missing token ' UNK '\n",
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 253 ms\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# get length of each essay and its title\n",
    "df['essay1_len'] = df['project_essay_1'].apply(len)\n",
    "df['essay2_len'] = df['project_essay_2'].apply(len)\n",
    "df['essay3_len'] = df['project_essay_3'].apply(len)\n",
    "df['essay4_len'] = df['project_essay_4'].apply(len)\n",
    "df['title_len'] = df['project_title'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.4 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# Combine the essays into one string\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']),\n",
    "                                            str(row['project_essay_2']),\n",
    "                                            str(row['project_essay_3']),\n",
    "                                            str(row['project_essay_4'])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 46s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# get our delicious features from that massive text\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count'] + 1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP style features\n",
    "+ Article Polarity - Sentiment polarity\n",
    "+ Article Subjectivity - Sentiment subjectivity\n",
    "+ Noun Count - count of words that are nouns, the ones that name objects, people, etc...\n",
    "+ Verb Count - count of words that are verbs, the ones that tell you about moving like walk or think...\n",
    "+ Adjective Count - count of words that are adjectives, the ones that describe nouns like red or big...\n",
    "+ Adverb Count - count of words that are adverbs, the ones that describe adjectives or verbs and typically end with -ly\n",
    "+ Pronoun Count - count of words that are pronouns, the ones that replace other words like her or they"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\" target=\"_blank\">NLTK Part of Speech Tags</a> <- Click me (don't get excited there's no R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def blob_the_text(text):\n",
    "    \"\"\"\n",
    "    take in a text and apply a bunch of text blob features to it\n",
    "    INPUT: text string\n",
    "    OUTPUT: a tuple of everything you might want textblob to run on that text\n",
    "            sentiment polarity,\n",
    "            sentiment subjectivity,\n",
    "            count of nouns,\n",
    "            count of pronouns,\n",
    "            count of verbs,\n",
    "            count of adjectives,\n",
    "            count of adverbs\n",
    "    \"\"\"\n",
    "    tb = TextBlob(text)\n",
    "\n",
    "    nouns = ['NN', 'NNS', 'NNP', 'NNPS'] #singular, plural regular nouns, singular, plural proper nouns\n",
    "    pronouns = ['PRP', 'PRP$', 'WP', 'WP$'] #personal pronouns, possessive personal, wh pronouns, possessive wh pronouns\n",
    "    verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] #verb base, past tense, gerund, past participle, singular present, 3rd person present\n",
    "    adjectives = ['JJ', 'JJR', 'JJS'] #adjective, comparative, superlative\n",
    "    adverbs = ['RB', 'RBR', 'RBS', 'WRB'] #adverb, compartive, superlative, wh- adverb\n",
    "\n",
    "    tagcol = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    tags = [tagcol(word[0], word[1]) for word in tb.tags]\n",
    "\n",
    "    try:\n",
    "        pol = tb.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    try:\n",
    "        subj = tb.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    ncount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in nouns).values())\n",
    "    procount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in pronouns).values())\n",
    "    vcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in verbs).values())\n",
    "    adjcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adjectives).values())\n",
    "    advcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adverbs).values())\n",
    "\n",
    "    # print('polarity {} subjectivity {}'.format(pol, subj))\n",
    "    # print('pos tags: {}'.format(posstring))\n",
    "    # trying = TextBlob(df['text'][0]).tags\n",
    "    # tag = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    # tags = [tag(thing[0], thing[1]) for thing in trying]\n",
    "    return [pol, subj, ncount, procount, vcount, adjcount, advcount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 3min 49s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "df[['polarity', 'subjectivity', 'noun_count', 'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count']] = pd.DataFrame([(blob_the_text(row['text'])) for index, row in df.iterrows()], index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>adverb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>0.213402</td>\n",
       "      <td>0.391136</td>\n",
       "      <td>81</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>0.192889</td>\n",
       "      <td>0.597111</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>0.353888</td>\n",
       "      <td>0.534450</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>0.175880</td>\n",
       "      <td>0.416224</td>\n",
       "      <td>105</td>\n",
       "      <td>34</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>0.285417</td>\n",
       "      <td>0.557192</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  project_grade_category  polarity  subjectivity  noun_count  pronoun_count  \\\n",
       "0          Grades PreK-2  0.213402      0.391136          81             36   \n",
       "1             Grades 3-5  0.192889      0.597111          59             19   \n",
       "2             Grades 3-5  0.353888      0.534450          58             32   \n",
       "3             Grades 3-5  0.175880      0.416224         105             34   \n",
       "4             Grades 6-8  0.285417      0.557192          44             23   \n",
       "\n",
       "   verb_count  adjective_count  adverb_count  \n",
       "0          58               25            16  \n",
       "1          27               25            10  \n",
       "2          51               19             7  \n",
       "3          78               37            23  \n",
       "4          33               18             9  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['project_grade_category', 'polarity', 'subjectivity', 'noun_count', 'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count']].head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.4 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "df.to_csv('../data/training_clean.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/training_clean.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make that all into a nice little file that will hopefully work nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clean_and_feature.py\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%file clean_and_feature.py\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def blob_the_text(text):\n",
    "    \"\"\"\n",
    "    take in a text and apply a bunch of text blob features to it\n",
    "    INPUT: text string\n",
    "    OUTPUT: a tuple of everything you might want textblob to run on that text\n",
    "            sentiment polarity,\n",
    "            sentiment subjectivity,\n",
    "            count of nouns,\n",
    "            count of pronouns,\n",
    "            count of verbs,\n",
    "            count of adjectives,\n",
    "            count of adverbs\n",
    "    \"\"\"\n",
    "    tb = TextBlob(text)\n",
    "\n",
    "    nouns = ['NN', 'NNS', 'NNP', 'NNPS'] #singular, plural regular nouns, singular, plural proper nouns\n",
    "    pronouns = ['PRP', 'PRP$', 'WP', 'WP$'] #personal pronouns, possessive personal, wh pronouns, possessive wh pronouns\n",
    "    verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] #verb base, past tense, gerund, past participle, singular present, 3rd person present\n",
    "    adjectives = ['JJ', 'JJR', 'JJS'] #adjective, comparative, superlative\n",
    "    adverbs = ['RB', 'RBR', 'RBS', 'WRB'] #adverb, compartive, superlative, wh- adverb\n",
    "\n",
    "    tagcol = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    tags = [tagcol(word[0], word[1]) for word in tb.tags]\n",
    "\n",
    "    try:\n",
    "        pol = tb.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    try:\n",
    "        subj = tb.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    ncount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in nouns).values())\n",
    "    procount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in pronouns).values())\n",
    "    vcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in verbs).values())\n",
    "    adjcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adjectives).values())\n",
    "    advcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adverbs).values())\n",
    "\n",
    "    # print('polarity {} subjectivity {}'.format(pol, subj))\n",
    "    # print('pos tags: {}'.format(posstring))\n",
    "    # trying = TextBlob(df['text'][0]).tags\n",
    "    # tag = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    # tags = [tag(thing[0], thing[1]) for thing in trying]\n",
    "    return [pol, subj, ncount, procount, vcount, adjcount, advcount]\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# declare some strings\n",
    "id_column = 'id'\n",
    "missing_token = ' UNK '\n",
    "\n",
    "# read in our data, parse_dates=['column name'] will read that column as a datetime object, can take a boolean, list of integers / names, list of lists or a dictionary,\n",
    "# does different things depending on which one you use read the docs~\n",
    "train = pd.read_csv('data/train.csv', parse_dates=['project_submitted_datetime'])\n",
    "# test = pd.read_csv('data/test.csv', parse_dates=['project_submitted_datetime'])\n",
    "hopes = pd.read_csv('data/resources.csv').fillna(missing_token)\n",
    "\n",
    "# # lets make a master df of the train and test data to make our lives easier!\n",
    "# df = pd.concat([train,test], axis=0)\n",
    "# no lets not that was awful\n",
    "df = train\n",
    "\n",
    "# A new column for total price\n",
    "hopes['total_price'] = hopes['quantity']*hopes['price']\n",
    "\n",
    "# Make an aggregate df to join to our normal df\n",
    "# the .agg method takes in a function, string, or a dictionary or list of strings or functions.  The dictionary keys will be column names upon which functions should be run\n",
    "# I named it after the horse in Shadow of the Colossus~ the description column is now a count of how many, so it can be renamed to (number of )items\n",
    "agro = {'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}\n",
    "aggregatedf = hopes.groupby('id').agg(agro).rename(columns={'description':'items'})\n",
    "\n",
    "# now lets use that string functionality of .agg to get the min, max, and mean values!\n",
    "for maths in ['min', 'max', 'mean']:\n",
    "    # romanized Japanese horse name from game, and that guy that changes names in ff because why not lets have fun with variable names they're just for here anyway\n",
    "    aguro = {'quantity':maths, 'price':maths, 'total_price':maths}\n",
    "    namingway = {'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}\n",
    "    \n",
    "    # do some aggregation and join it to our previously created df\n",
    "    temporary = hopes.groupby('id').agg(aguro).rename(columns=namingway).fillna(0)\n",
    "    aggregatedf = aggregatedf.join(temporary)\n",
    "# This didn't work whoops # aggregatedf = aggregatedf.join([hopes.groupby('id').agg({'quantity':maths, 'price':maths, 'total_price':maths}).rename(columns={'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}).fillna(0) for maths in ['min', 'max', 'mean']])\n",
    "\n",
    "# and finally give it the original description columns aggregated together with a space in between them\n",
    "aggregatedf = aggregatedf.join(hopes.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n",
    "\n",
    "# Join that together with our everything df and check it out\n",
    "df = df.join(aggregatedf, on='id')\n",
    "df.head()\n",
    "\n",
    "# using datetime to make the above features\n",
    "df['year'] = df['project_submitted_datetime'].dt.year\n",
    "df['month'] = df['project_submitted_datetime'].dt.month\n",
    "df['year_day'] = df['project_submitted_datetime'].dt.dayofyear\n",
    "df['month_day'] = df['project_submitted_datetime'].dt.day\n",
    "df['week_day'] = df['project_submitted_datetime'].dt.weekday\n",
    "df['hour'] = df['project_submitted_datetime'].dt.hour\n",
    "\n",
    "# fill empty values with missing token ' UNK '\n",
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)\n",
    "\n",
    "# get length of each essay and its title\n",
    "df['essay1_len'] = df['project_essay_1'].apply(len)\n",
    "df['essay2_len'] = df['project_essay_2'].apply(len)\n",
    "df['essay3_len'] = df['project_essay_3'].apply(len)\n",
    "df['essay4_len'] = df['project_essay_4'].apply(len)\n",
    "df['title_len'] = df['project_title'].apply(len)\n",
    "\n",
    "# Combine the essays into one string\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']),\n",
    "                                            str(row['project_essay_2']),\n",
    "                                            str(row['project_essay_3']),\n",
    "                                            str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "# get our delicious features from that massive text\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count'] + 1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "\n",
    "df[['polarity', 'subjectivity', 'noun_count', 'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count']] = pd.DataFrame([(blob_the_text(row['text'])) for index, row in df.iterrows()], index = df.index)\n",
    "\n",
    "df.to_csv('../data/training_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF style features\n",
    "+ 1-3 NGram TF-IDF for Article Text at word level\n",
    "+ 1-3 NGram TF-IDF for Project Title at word level\n",
    "+ 1-3 NGram TF-IDF for Resource Text at word level\n",
    "+ 1-3 NGram TF-IDF for Article Text at character level\n",
    "+ 1-3 NGram TF-IDF for Project Title at character level\n",
    "+ 1-3 NGram TF-IDF for Resource Text at character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/training_clean.csv', encoding='utf-8').fillna(missing_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['teacher_number_of_previously_posted_projects', 'project_is_approved', 'items', 'quantity', 'price', 'total_price', 'min_quantity', 'min_price', 'min_total_price',\n",
    "          'max_quantity', 'max_price', 'max_total_price', 'year', 'month', 'year_day', 'month_day', 'week_day', 'hour', 'essay1_len', 'essay2_len', 'essay3_len', 'essay4_len',\n",
    "          'title_len', 'char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'stopword_count', 'polarity', 'subjectivity', 'noun_count',\n",
    "          'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count', 'text', 'resource_description', 'project_resource_summary', 'project_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.55 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "X = train.drop('project_is_approved', axis=1)\n",
    "y = train['project_is_approved']\n",
    "\n",
    "X['resource_text'] = X.apply(lambda row: ' '.join([str(row['resource_description']), str(row['project_resource_summary'])]), axis=1)\n",
    "X = X.drop(['resource_description','project_resource_summary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
=======
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30 ms\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "\n",
    "article_text = list(X_train['text'].values)\n",
    "title_text = list(X_train['project_title'].values)\n",
    "resource_text = list(X_train['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 47s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "article_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "article_vectorizer.fit(article_text)\n",
    "article_word_tfidf = article_vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.03 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.5 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "resource_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "resource_vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = resource_vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 294 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = X_train.drop(['text', 'project_title', 'resource_text'], axis=1)\n",
    "extra = sp.sparse.csr_matrix(X_train.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136560, 35)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136560, 2535)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trained = sp.sparse.hstack((article_word_tfidf, extra))\n",
    "X_trained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for test group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.9 ms\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "\n",
    "article_text = list(X_test['text'].values)\n",
    "title_text = list(X_test['project_title'].values)\n",
    "resource_text = list(X_test['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "article_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "article_vectorizer.fit(article_text)\n",
    "article_word_tfidf = article_vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "resource_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "resource_vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = resource_vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = X_test.drop(['text', 'project_title', 'resource_text'], axis=1)\n",
    "extra = sp.sparse.csr_matrix(X_test.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45520, 2535)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tested = sp.sparse.hstack((article_word_tfidf, extra))\n",
    "X_tested.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1 Score: 0.846572934973638\n",
      "C: 10 Score: 0.846572934973638\n",
      "C: 100 Score: 0.846572934973638\n",
      "C: 1000 Score: 0.8465949033391915\n",
      "C: 10000.0 Score: 0.8466608084358523\n",
      "Wall time: 6min 10s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "for C in [1, 10, 100, 1000, 1e4]:\n",
    "    logreg = LogisticRegression(C=C)\n",
    "    logreg.fit(X_trained, y_train)\n",
    "    y_pred_class = logreg.predict(X_tested)\n",
    "\n",
    "    print('C: {} Score: {}'.format(C, metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and once more for everything"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "article_text = list(X['text'].values)\n",
    "title_text = list(X['project_title'].values)\n",
    "resource_text = list(X['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 42s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "article_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "article_vectorizer.fit(article_text)\n",
    "article_word_tfidf = article_vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.38 s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "resource_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "resource_vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = resource_vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 374 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = X.drop(['text', 'project_title', 'resource_text'], axis=1)\n",
    "extra = sp.sparse.csr_matrix(X.astype(float))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:data cleaning and model training/.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "execution_count": null,
=======
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182080, 2535)\n",
      "(182080, 5035)\n",
      "(182080, 7535)\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_training_arc = sp.sparse.hstack((article_word_tfidf, extra))\n",
    "print(X_training_arc.shape)\n",
    "X_training_arc = sp.sparse.hstack((title_word_tfidf, X_training_arc))\n",
    "print(X_training_arc.shape)\n",
    "X_training_arc = sp.sparse.hstack((resource_word_tfidf, X_training_arc))\n",
    "print(X_training_arc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_cat = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182080, 7535)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training_arc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=35))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 'batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "182080/182080 [==============================] - 8s 43us/step - loss: 2.4545 - acc: 0.8475\n",
      "Epoch 2/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 3/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 4/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 5/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 6/100\n",
      "182080/182080 [==============================] - 6s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 7/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 8/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 9/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 10/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 11/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 12/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 13/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 14/100\n",
      "182080/182080 [==============================] - 7s 41us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 15/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 16/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 17/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 18/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 19/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 20/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 21/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 22/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 23/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 24/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 25/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 26/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 27/100\n",
      "182080/182080 [==============================] - 6s 35us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 28/100\n",
      "182080/182080 [==============================] - 6s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 29/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 30/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 31/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 32/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 33/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 34/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 35/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 36/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 37/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 38/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 39/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 40/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 41/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 42/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 43/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 44/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 45/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 46/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 47/100\n",
      "182080/182080 [==============================] - 7s 38us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 48/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 49/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 50/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 51/100\n",
      "182080/182080 [==============================] - 7s 39us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 52/100\n",
      "182080/182080 [==============================] - 7s 38us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 53/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 54/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 55/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 56/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 57/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 58/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 59/100\n",
      "182080/182080 [==============================] - 7s 38us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 60/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 61/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 62/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 63/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 64/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 65/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 66/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 67/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 68/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 69/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 70/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 71/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 72/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 73/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 74/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 75/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 76/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 77/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 78/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 79/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 80/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 81/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 82/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 83/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 84/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 85/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 86/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 87/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 88/100\n",
      "182080/182080 [==============================] - 7s 37us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 89/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 90/100\n",
      "182080/182080 [==============================] - 7s 38us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 91/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 92/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 93/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 94/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 95/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 96/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 97/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 98/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 99/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n",
      "Epoch 100/100\n",
      "182080/182080 [==============================] - 7s 36us/step - loss: 2.4551 - acc: 0.8477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27d879b2400>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X,\n",
    "    y_cat,\n",
    "    shuffle=True,\n",
    "    epochs=100,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For whatever reason, this model has the same accuracy as a logistic regression.  Look into this.\n",
    "Probably because it wasn't accepting my other input because I did a thing wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('too_many_cooks.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
>>>>>>> kevin:.ipynb_checkpoints/cleaning-checkpoint.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create a dictionary mapping tokens to their tfidf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['resource_word_tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['resource_word_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 15 highest tf-idf from that list\n",
    "tfidf.sort_values(by=['resource_word_tfidf'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Character level tf-idfs\n",
    "# article text\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(article_text)\n",
    "article_char_tfidf = vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# project title\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_char_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# resource text\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(resource_text)\n",
    "resource_char_tfidf = vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Be Continued...  My feeble attempts that weren't anywhere near all encompassing are below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athing = resource_df[resource_df['id'] == 'p069063']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athing_length = len(athing)\n",
    "for row in athing.itertuples():\n",
    "    print(round(row[3] * row[4], 2))\n",
    "athing_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumprice = []\n",
    "numbought = []\n",
    "avgprice = []\n",
    "\n",
    "for row in train_df.itertuples():\n",
    "    try:\n",
    "        df = resource_df[resource_df['id'] == row[1]]\n",
    "        df_length = len(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resource_scrape(idnum):\n",
    "    df = resource_df[resource_df['id'] == idnum]\n",
    "    try:\n",
    "        foo = [round(row[3] * row[4], 2) for row in df.itertuples()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['project_is_approved'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['teacher_number_of_previously_posted_projects'].value_counts() > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
