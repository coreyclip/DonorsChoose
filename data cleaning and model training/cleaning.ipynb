{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong>In This Notebook...</strong></h2><br />\n",
    "This is for data cleaning and engineering for our project.  Much inspiration received from <a href=\"https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering/notebook\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Input, Embedding\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# declare some strings\n",
    "id_column = 'id'\n",
    "missing_token = ' UNK '\n",
    "\n",
    "# read in our data, parse_dates=['column name'] will read that column as a datetime object, can take a boolean, list of integers / names, list of lists or a dictionary,\n",
    "# does different things depending on which one you use read the docs~\n",
    "train = pd.read_csv('H:/uci_data/donorschoose-application-screening/train/train.csv', parse_dates=['project_submitted_datetime'])\n",
    "test = pd.read_csv('H:/uci_data/donorschoose-application-screening/test/test.csv', parse_dates=['project_submitted_datetime'])\n",
    "hopes = pd.read_csv('H:/uci_data/donorschoose-application-screening/resources/resources.csv').fillna(missing_token)\n",
    "\n",
    "# lets make a master df of the train and test data to make our lives easier!\n",
    "df = pd.concat([train,test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathy Features\n",
    "+ Min, Max, Mean Price for resources requested\n",
    "+ Min Quantity, Max Quantity, Mean Quantity of resources requested\n",
    "+ Min Total Price, Max Total Price, Mean Total Price of resources requested\n",
    "+ Total Price of items requested by proposal\n",
    "+ Number of Unique Items Requested by proposal\n",
    "+ Quantity of items requested in proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# A new column for total price\n",
    "hopes['total_price'] = hopes['quantity']*hopes['price']\n",
    "\n",
    "# Make an aggregate df to join to our normal df\n",
    "# the .agg method takes in a function, string, or a dictionary or list of strings or functions.  The dictionary keys will be column names upon which functions should be run\n",
    "# I named it after the horse in Shadow of the Colossus~ the description column is now a count of how many, so it can be renamed to (number of )items\n",
    "agro = {'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}\n",
    "aggregatedf = hopes.groupby('id').agg(agro).rename(columns={'description':'items'})\n",
    "\n",
    "# now lets use that string functionality of .agg to get the min, max, and mean values!\n",
    "for maths in ['min', 'max', 'mean']:\n",
    "    # romanized Japanese horse name from game, and that guy that changes names in ff because why not lets have fun with variable names they're just for here anyway\n",
    "    aguro = {'quantity':maths, 'price':maths, 'total_price':maths}\n",
    "    namingway = {'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}\n",
    "    \n",
    "    # do some aggregation and join it to our previously created df\n",
    "    temporary = hopes.groupby('id').agg(aguro).rename(columns=namingway).fillna(0)\n",
    "    aggregatedf = aggregatedf.join(temporary)\n",
    "# This didn't work whoops # aggregatedf = aggregatedf.join([hopes.groupby('id').agg({'quantity':maths, 'price':maths, 'total_price':maths}).rename(columns={'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}).fillna(0) for maths in ['min', 'max', 'mean']])\n",
    "\n",
    "# and finally give it the original description columns aggregated together with a space in between them\n",
    "aggregatedf = aggregatedf.join(hopes.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n",
    "\n",
    "# Join that together with our everything df and check it out\n",
    "df = df.join(aggregatedf, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great, now lets play with time!\n",
    "+ Year of submission\n",
    "+ Month of submission\n",
    "+ Year Day (1-365) of submission\n",
    "+ Month Day (1-31) of submission\n",
    "+ Week Day (1-7) of submission\n",
    "+ Hour of submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# using datetime to make the above features\n",
    "df['Year'] = df['project_submitted_datetime'].dt.year\n",
    "df['Month'] = df['project_submitted_datetime'].dt.month\n",
    "df['Year_Day'] = df['project_submitted_datetime'].dt.dayofyear\n",
    "df['Month_Day'] = df['project_submitted_datetime'].dt.day\n",
    "df['Week_Day'] = df['project_submitted_datetime'].dt.weekday\n",
    "df['Hour'] = df['project_submitted_datetime'].dt.hour\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text based features\n",
    "+ Length of essays including spaces\n",
    "+ Length of project title\n",
    "+ Word count across essays\n",
    "+ Character count across essays\n",
    "+ Word density / average length of words used\n",
    "+ Punctuation count\n",
    "+ Uppercase count\n",
    "+ Title Word Count (Gotta Have This Case)\n",
    "+ Stopword Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill empty values with missing token ' UNK '\n",
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get length of each essay and its title\n",
    "df['essay1_len'] = df['project_essay_1'].apply(len)\n",
    "df['essay2_len'] = df['project_essay_2'].apply(len)\n",
    "df['essay3_len'] = df['project_essay_3'].apply(len)\n",
    "df['essay4_len'] = df['project_essay_4'].apply(len)\n",
    "df['title_len'] = df['project_title'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine the essays into one string\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']),\n",
    "                                            str(row['project_essay_2']),\n",
    "                                            str(row['project_essay_3']),\n",
    "                                            str(row['project_essay_4'])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get our delicious features from that massive text\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word density'] = df['char_count'] / (df['word_count'] + 1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP style features\n",
    "+ Article Polarity - Sentiment polarity\n",
    "+ Article Subjectivity - Sentiment subjectivity\n",
    "+ Noun Count - count of words that are nouns, the ones that name objects, people, etc...\n",
    "+ Verb Count - count of words that are verbs, the ones that tell you about moving like walk or think...\n",
    "+ Adjective Count - count of words that are adjectives, the ones that describe nouns like red or big...\n",
    "+ Adverb Count - count of words that are adverbs, the ones that describe adjectives or verbs and typically end with -ly\n",
    "+ Pronoun Count - count of words that are pronouns, the ones that replace other words like her or they"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# functions get polarity and subjectivity using TextBlob\n",
    "def get_polarity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "        pol = textblob.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    return pol\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    try:\n",
    "        textblob = TextBloob(unicode(text, 'utf-8'))\n",
    "        subj = textblob.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    return subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 318 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now lets apply those functions to our df\n",
    "df['polarity'] = df['text'].apply(get_polarity)\n",
    "df['subjectivity'] = df['text'].apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\" target=\"_blank\">NLTK Part of Speech Tags</a> <- Click me (don't get excited there's no R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make a dictionary for parts of speech\n",
    "pos_dict = {\n",
    "    'noun': ['NN', 'NNS', 'NNP', 'NNPS'], #singular, plural regular nouns, singular, plural proper nouns\n",
    "    'pron': ['PRP', 'PRP$', 'WP', 'WP$'], #personal pronouns, possessive personal, wh pronouns, possessive wh pronouns\n",
    "    'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], #verb base, past tense, gerund, past participle, singular present, 3rd person present\n",
    "    'adj': ['JJ', 'JJR', 'JJS'], #adjective, comparative, superlative\n",
    "    'adv': ['RB', 'RBR', 'RBS', 'WRB'] #adverb, compartive, superlative, wh- adverb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# function to retrieve the parts of speech tag counts\n",
    "def pos_check(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_dic[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets use that function to make new columns each in their own cell because it takes a while\n",
    "df['noun_count'] = df['text'].apply(lambda x: pos_check(x, 'noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['verb_count'] = df['text'].apply(lambda x: pos_check(x, 'verb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['adj_count'] = df['text'].apply(lambda x: pos_check(x, 'adj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['adv_count'] = df['text'].apply(lambda x: pos_check(x, 'adv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['pron_count'] = df['text'].apply(lambda x: pos_check(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF style features\n",
    "+ 1-3 NGram TF-IDF for Article Text at word level\n",
    "+ 1-3 NGram TF-IDF for Project Title at word level\n",
    "+ 1-3 NGram TF-IDF for Resource Text at word level\n",
    "+ 1-3 NGram TF-IDF for Article Text at character level\n",
    "+ 1-3 NGram TF-IDF for Project Title at character level\n",
    "+ 1-3 NGram TF-IDF for Resource Text at character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['resource_text'] = df.apply(lambda row: ' '.join([str(row['resource_description']), str(row['project_resource_summary'])]), axis=1)\n",
    "\n",
    "article_text = list(df['text'].values)\n",
    "title_text = list(df['project_title'].values)\n",
    "resource_text = list(df['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_csv('H:/uci_data/donorschoose-application-screening/everything.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(article_text)\n",
    "article_word_tfidf = vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create a dictionary mapping tokens to their tfidf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['resource_word_tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['resource_word_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 15 highest tf-idf from that list\n",
    "tfidf.sort_values(by=['resource_word_tfidf'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Character level tf-idfs\n",
    "# article text\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(article_text)\n",
    "article_char_tfidf = vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# project title\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_char_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# resource text\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(resource_text)\n",
    "resource_char_tfidf = vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Be Continued...  My feeble attempts that weren't anywhere near all encompassing are below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athing = resource_df[resource_df['id'] == 'p069063']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athing_length = len(athing)\n",
    "for row in athing.itertuples():\n",
    "    print(round(row[3] * row[4], 2))\n",
    "athing_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumprice = []\n",
    "numbought = []\n",
    "avgprice = []\n",
    "\n",
    "for row in train_df.itertuples():\n",
    "    try:\n",
    "        df = resource_df[resource_df['id'] == row[1]]\n",
    "        df_length = len(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resource_scrape(idnum):\n",
    "    df = resource_df[resource_df['id'] == idnum]\n",
    "    try:\n",
    "        foo = [round(row[3] * row[4], 2) for row in df.itertuples()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['project_is_approved'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['teacher_number_of_previously_posted_projects'].value_counts() > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
