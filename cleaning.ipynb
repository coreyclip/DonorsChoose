{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong>In This Notebook...</strong></h2><br />\n",
    "This is for data cleaning and engineering for our project.  Much inspiration received from <a href=\"https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering/notebook\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Input, Embedding\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import warnings\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# declare some strings\n",
    "id_column = 'id'\n",
    "missing_token = ' UNK '\n",
    "\n",
    "# read in our data, parse_dates=['column name'] will read that column as a datetime object, can take a boolean, list of integers / names, list of lists or a dictionary,\n",
    "# does different things depending on which one you use read the docs~\n",
    "train = pd.read_csv('../data/train.csv', parse_dates=['project_submitted_datetime'])\n",
    "test = pd.read_csv('../data/test.csv', parse_dates=['project_submitted_datetime'])\n",
    "hopes = pd.read_csv('../data/resources.csv').fillna(missing_token)\n",
    "\n",
    "# # lets make a master df of the train and test data to make our lives easier!\n",
    "# df = pd.concat([train,test], axis=0)\n",
    "# no lets not that was awful\n",
    "df = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathy Features\n",
    "+ Min, Max, Mean Price for resources requested\n",
    "+ Min Quantity, Max Quantity, Mean Quantity of resources requested\n",
    "+ Min Total Price, Max Total Price, Mean Total Price of resources requested\n",
    "+ Total Price of items requested by proposal\n",
    "+ Number of Unique Items Requested by proposal\n",
    "+ Quantity of items requested in proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# A new column for total price\n",
    "hopes['total_price'] = hopes['quantity']*hopes['price']\n",
    "\n",
    "# Make an aggregate df to join to our normal df\n",
    "# the .agg method takes in a function, string, or a dictionary or list of strings or functions.  The dictionary keys will be column names upon which functions should be run\n",
    "# I named it after the horse in Shadow of the Colossus~ the description column is now a count of how many, so it can be renamed to (number of )items\n",
    "agro = {'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}\n",
    "aggregatedf = hopes.groupby('id').agg(agro).rename(columns={'description':'items'})\n",
    "\n",
    "# now lets use that string functionality of .agg to get the min, max, and mean values!\n",
    "for maths in ['min', 'max', 'mean']:\n",
    "    # romanized Japanese horse name from game, and that guy that changes names in ff because why not lets have fun with variable names they're just for here anyway\n",
    "    aguro = {'quantity':maths, 'price':maths, 'total_price':maths}\n",
    "    namingway = {'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}\n",
    "    \n",
    "    # do some aggregation and join it to our previously created df\n",
    "    temporary = hopes.groupby('id').agg(aguro).rename(columns=namingway).fillna(0)\n",
    "    aggregatedf = aggregatedf.join(temporary)\n",
    "# This didn't work whoops # aggregatedf = aggregatedf.join([hopes.groupby('id').agg({'quantity':maths, 'price':maths, 'total_price':maths}).rename(columns={'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}).fillna(0) for maths in ['min', 'max', 'mean']])\n",
    "\n",
    "# and finally give it the original description columns aggregated together with a space in between them\n",
    "aggregatedf = aggregatedf.join(hopes.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n",
    "\n",
    "# Join that together with our everything df and check it out\n",
    "df = df.join(aggregatedf, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great, now lets play with time!\n",
    "+ Year of submission\n",
    "+ Month of submission\n",
    "+ Year Day (1-365) of submission\n",
    "+ Month Day (1-31) of submission\n",
    "+ Week Day (1-7) of submission\n",
    "+ Hour of submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# using datetime to make the above features\n",
    "df['year'] = df['project_submitted_datetime'].dt.year\n",
    "df['month'] = df['project_submitted_datetime'].dt.month\n",
    "df['year_day'] = df['project_submitted_datetime'].dt.dayofyear\n",
    "df['month_day'] = df['project_submitted_datetime'].dt.day\n",
    "df['week_day'] = df['project_submitted_datetime'].dt.weekday\n",
    "df['hour'] = df['project_submitted_datetime'].dt.hour\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text based features\n",
    "+ Length of essays including spaces\n",
    "+ Length of project title\n",
    "+ Word count across essays\n",
    "+ Character count across essays\n",
    "+ Word density / average length of words used\n",
    "+ Punctuation count\n",
    "+ Uppercase count\n",
    "+ Title Word Count (Gotta Have This Case)\n",
    "+ Stopword Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill empty values with missing token ' UNK '\n",
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 230 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get length of each essay and its title\n",
    "df['essay1_len'] = df['project_essay_1'].apply(len)\n",
    "df['essay2_len'] = df['project_essay_2'].apply(len)\n",
    "df['essay3_len'] = df['project_essay_3'].apply(len)\n",
    "df['essay4_len'] = df['project_essay_4'].apply(len)\n",
    "df['title_len'] = df['project_title'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine the essays into one string\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']),\n",
    "                                            str(row['project_essay_2']),\n",
    "                                            str(row['project_essay_3']),\n",
    "                                            str(row['project_essay_4'])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get our delicious features from that massive text\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count'] + 1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP style features\n",
    "+ Article Polarity - Sentiment polarity\n",
    "+ Article Subjectivity - Sentiment subjectivity\n",
    "+ Noun Count - count of words that are nouns, the ones that name objects, people, etc...\n",
    "+ Verb Count - count of words that are verbs, the ones that tell you about moving like walk or think...\n",
    "+ Adjective Count - count of words that are adjectives, the ones that describe nouns like red or big...\n",
    "+ Adverb Count - count of words that are adverbs, the ones that describe adjectives or verbs and typically end with -ly\n",
    "+ Pronoun Count - count of words that are pronouns, the ones that replace other words like her or they"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\" target=\"_blank\">NLTK Part of Speech Tags</a> <- Click me (don't get excited there's no R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def blob_the_text(text):\n",
    "    \"\"\"\n",
    "    take in a text and apply a bunch of text blob features to it\n",
    "    INPUT: text string\n",
    "    OUTPUT: a tuple of everything you might want textblob to run on that text\n",
    "            sentiment polarity,\n",
    "            sentiment subjectivity,\n",
    "            count of nouns,\n",
    "            count of pronouns,\n",
    "            count of verbs,\n",
    "            count of adjectives,\n",
    "            count of adverbs\n",
    "    \"\"\"\n",
    "    tb = TextBlob(text)\n",
    "\n",
    "    nouns = ['NN', 'NNS', 'NNP', 'NNPS'] #singular, plural regular nouns, singular, plural proper nouns\n",
    "    pronouns = ['PRP', 'PRP$', 'WP', 'WP$'] #personal pronouns, possessive personal, wh pronouns, possessive wh pronouns\n",
    "    verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] #verb base, past tense, gerund, past participle, singular present, 3rd person present\n",
    "    adjectives = ['JJ', 'JJR', 'JJS'] #adjective, comparative, superlative\n",
    "    adverbs = ['RB', 'RBR', 'RBS', 'WRB'] #adverb, compartive, superlative, wh- adverb\n",
    "\n",
    "    tagcol = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    tags = [tagcol(word[0], word[1]) for word in tb.tags]\n",
    "\n",
    "    try:\n",
    "        pol = tb.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    try:\n",
    "        subj = tb.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    ncount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in nouns).values())\n",
    "    procount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in pronouns).values())\n",
    "    vcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in verbs).values())\n",
    "    adjcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adjectives).values())\n",
    "    advcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adverbs).values())\n",
    "\n",
    "    # print('polarity {} subjectivity {}'.format(pol, subj))\n",
    "    # print('pos tags: {}'.format(posstring))\n",
    "    # trying = TextBlob(df['text'][0]).tags\n",
    "    # tag = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    # tags = [tag(thing[0], thing[1]) for thing in trying]\n",
    "    return [pol, subj, ncount, procount, vcount, adjcount, advcount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[['polarity', 'subjectivity', 'noun_count', 'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count']] = pd.DataFrame([(blob_the_text(row['text'])) for index, row in df.iterrows()], index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>adverb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>0.213402</td>\n",
       "      <td>0.391136</td>\n",
       "      <td>81</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>0.192889</td>\n",
       "      <td>0.597111</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>0.353888</td>\n",
       "      <td>0.534450</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>0.175880</td>\n",
       "      <td>0.416224</td>\n",
       "      <td>105</td>\n",
       "      <td>34</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>0.285417</td>\n",
       "      <td>0.557192</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  project_grade_category  polarity  subjectivity  noun_count  pronoun_count  \\\n",
       "0          Grades PreK-2  0.213402      0.391136          81             36   \n",
       "1             Grades 3-5  0.192889      0.597111          59             19   \n",
       "2             Grades 3-5  0.353888      0.534450          58             32   \n",
       "3             Grades 3-5  0.175880      0.416224         105             34   \n",
       "4             Grades 6-8  0.285417      0.557192          44             23   \n",
       "\n",
       "   verb_count  adjective_count  adverb_count  \n",
       "0          58               25            16  \n",
       "1          27               25            10  \n",
       "2          51               19             7  \n",
       "3          78               37            23  \n",
       "4          33               18             9  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['project_grade_category', 'polarity', 'subjectivity', 'noun_count', 'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # functions get polarity and subjectivity using TextBlob\n",
    "# def get_polarity(text):\n",
    "#     try:\n",
    "#         textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "#         pol = textblob.sentiment.polarity\n",
    "#     except:\n",
    "#         pol = 0.0\n",
    "#     return pol\n",
    "\n",
    "# def get_subjectivity(text):\n",
    "#     try:\n",
    "#         textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "#         subj = textblob.sentiment.subjectivity\n",
    "#     except:\n",
    "#         subj = 0.0\n",
    "#     return subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 212 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Now lets apply those functions to our df\n",
    "# df['polarity'] = df['text'].apply(get_polarity)\n",
    "# df['subjectivity'] = df['text'].apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    182080\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # function to retrieve the parts of speech tag counts\n",
    "# def pos_check(x, flag):\n",
    "#     cnt = 0\n",
    "#     try:\n",
    "#         wiki = TextBlob(x)\n",
    "#         for tup in wiki.tags:\n",
    "#             ppo = list(tup)[1]\n",
    "#             if ppo in pos_dic[flag]:\n",
    "#                 cnt += 1\n",
    "#     except:\n",
    "#         pass\n",
    "#     return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_polarity(text):\n",
    "#     try:\n",
    "#         textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "#         pol = textblob.sentiment.polarity\n",
    "#     except:\n",
    "#         pol = 0.0\n",
    "#     return pol\n",
    "\n",
    "# def get_subjectivity(text):\n",
    "#     try:\n",
    "#         textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "#         subj = textblob.sentiment.subjectivity\n",
    "#     except:\n",
    "#         subj = 0.0\n",
    "#     return subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # make a dictionary for parts of speech\n",
    "# pos_dict = {\n",
    "#     'noun': ['NN', 'NNS', 'NNP', 'NNPS'], #singular, plural regular nouns, singular, plural proper nouns\n",
    "#     'pron': ['PRP', 'PRP$', 'WP', 'WP$'], #personal pronouns, possessive personal, wh pronouns, possessive wh pronouns\n",
    "#     'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], #verb base, past tense, gerund, past participle, singular present, 3rd person present\n",
    "#     'adj': ['JJ', 'JJR', 'JJS'], #adjective, comparative, superlative\n",
    "#     'adv': ['RB', 'RBR', 'RBS', 'WRB'] #adverb, compartive, superlative, wh- adverb\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 18min 6s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # now lets use that function to make new columns each in their own cell because it takes a while\n",
    "# df['noun_count'] = df['text'].apply(lambda x: pos_check(x, 'noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 17min 45s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# df['verb_count'] = df['text'].apply(lambda x: pos_check(x, 'verb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 17min 27s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# df['adj_count'] = df['text'].apply(lambda x: pos_check(x, 'adj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 17min 20s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# df['adv_count'] = df['text'].apply(lambda x: pos_check(x, 'adv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 17min 12s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# df['pron_count'] = df['text'].apply(lambda x: pos_check(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>...</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>adverb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p036502</td>\n",
       "      <td>484aaf11257089a66cfedc9461c6bd0a</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>NV</td>\n",
       "      <td>2016-11-18 14:45:59</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Super Sight Word Centers</td>\n",
       "      <td>Most of my kindergarten students come from low...</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "      <td>0.213402</td>\n",
       "      <td>0.391136</td>\n",
       "      <td>81</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p039565</td>\n",
       "      <td>df72a3ba8089423fa8a94be88060f6ed</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>GA</td>\n",
       "      <td>2017-04-26 15:57:28</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Music &amp; The Arts, Health &amp; Sports</td>\n",
       "      <td>Performing Arts, Team Sports</td>\n",
       "      <td>Keep Calm and Dance On</td>\n",
       "      <td>Our elementary school is a culturally rich sch...</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "      <td>0.192889</td>\n",
       "      <td>0.597111</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p233823</td>\n",
       "      <td>a9b876a9252e08a55e3d894150f75ba3</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>UT</td>\n",
       "      <td>2017-01-01 22:57:44</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Math &amp; Science, Literacy &amp; Language</td>\n",
       "      <td>Applied Sciences, Literature &amp; Writing</td>\n",
       "      <td>Lets 3Doodle to Learn</td>\n",
       "      <td>Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>103</td>\n",
       "      <td>0.353888</td>\n",
       "      <td>0.534450</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p185307</td>\n",
       "      <td>525fdbb6ec7f538a48beebaa0a51b24f</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>NC</td>\n",
       "      <td>2016-08-12 15:42:11</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "      <td>My students are the greatest students but are ...</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>188</td>\n",
       "      <td>0.175880</td>\n",
       "      <td>0.416224</td>\n",
       "      <td>105</td>\n",
       "      <td>34</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p013780</td>\n",
       "      <td>a63b5547a7239eae4c1872670848e61a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>CA</td>\n",
       "      <td>2016-08-06 09:09:11</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>We need clean water for our culinary arts class!</td>\n",
       "      <td>My students are athletes and students who are ...</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>0.285417</td>\n",
       "      <td>0.557192</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        teacher_id teacher_prefix school_state  \\\n",
       "0  p036502  484aaf11257089a66cfedc9461c6bd0a            Ms.           NV   \n",
       "1  p039565  df72a3ba8089423fa8a94be88060f6ed           Mrs.           GA   \n",
       "2  p233823  a9b876a9252e08a55e3d894150f75ba3            Ms.           UT   \n",
       "3  p185307  525fdbb6ec7f538a48beebaa0a51b24f            Mr.           NC   \n",
       "4  p013780  a63b5547a7239eae4c1872670848e61a            Mr.           CA   \n",
       "\n",
       "  project_submitted_datetime project_grade_category  \\\n",
       "0        2016-11-18 14:45:59          Grades PreK-2   \n",
       "1        2017-04-26 15:57:28             Grades 3-5   \n",
       "2        2017-01-01 22:57:44             Grades 3-5   \n",
       "3        2016-08-12 15:42:11             Grades 3-5   \n",
       "4        2016-08-06 09:09:11             Grades 6-8   \n",
       "\n",
       "            project_subject_categories  \\\n",
       "0                  Literacy & Language   \n",
       "1    Music & The Arts, Health & Sports   \n",
       "2  Math & Science, Literacy & Language   \n",
       "3                      Health & Sports   \n",
       "4                      Health & Sports   \n",
       "\n",
       "            project_subject_subcategories  \\\n",
       "0                                Literacy   \n",
       "1            Performing Arts, Team Sports   \n",
       "2  Applied Sciences, Literature & Writing   \n",
       "3                       Health & Wellness   \n",
       "4                       Health & Wellness   \n",
       "\n",
       "                                       project_title  \\\n",
       "0                           Super Sight Word Centers   \n",
       "1                             Keep Calm and Dance On   \n",
       "2                              Lets 3Doodle to Learn   \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...   \n",
       "4   We need clean water for our culinary arts class!   \n",
       "\n",
       "                                     project_essay_1      ...       \\\n",
       "0  Most of my kindergarten students come from low...      ...        \n",
       "1  Our elementary school is a culturally rich sch...      ...        \n",
       "2  Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...      ...        \n",
       "3  My students are the greatest students but are ...      ...        \n",
       "4  My students are athletes and students who are ...      ...        \n",
       "\n",
       "  title_word_count upper_case_word_count stopword_count  polarity  \\\n",
       "0               21                     7            151  0.213402   \n",
       "1               15                     5             79  0.192889   \n",
       "2               26                     6            103  0.353888   \n",
       "3               31                     6            188  0.175880   \n",
       "4               13                     2             98  0.285417   \n",
       "\n",
       "   subjectivity  noun_count  pronoun_count  verb_count  adjective_count  \\\n",
       "0      0.391136          81             36          58               25   \n",
       "1      0.597111          59             19          27               25   \n",
       "2      0.534450          58             32          51               19   \n",
       "3      0.416224         105             34          78               37   \n",
       "4      0.557192          44             23          33               18   \n",
       "\n",
       "   adverb_count  \n",
       "0            16  \n",
       "1            10  \n",
       "2             7  \n",
       "3            23  \n",
       "4             9  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.to_csv('../data/training_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make that all into a nice little file that will hopefully work nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clean_and_feature.py\n"
     ]
    }
   ],
   "source": [
    "%%file clean_and_feature.py\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Input, Embedding\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def blob_the_text(text):\n",
    "    \"\"\"\n",
    "    take in a text and apply a bunch of text blob features to it\n",
    "    INPUT: text string\n",
    "    OUTPUT: a tuple of everything you might want textblob to run on that text\n",
    "            sentiment polarity,\n",
    "            sentiment subjectivity,\n",
    "            count of nouns,\n",
    "            count of pronouns,\n",
    "            count of verbs,\n",
    "            count of adjectives,\n",
    "            count of adverbs\n",
    "    \"\"\"\n",
    "    tb = TextBlob(text)\n",
    "\n",
    "    nouns = ['NN', 'NNS', 'NNP', 'NNPS'] #singular, plural regular nouns, singular, plural proper nouns\n",
    "    pronouns = ['PRP', 'PRP$', 'WP', 'WP$'] #personal pronouns, possessive personal, wh pronouns, possessive wh pronouns\n",
    "    verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] #verb base, past tense, gerund, past participle, singular present, 3rd person present\n",
    "    adjectives = ['JJ', 'JJR', 'JJS'] #adjective, comparative, superlative\n",
    "    adverbs = ['RB', 'RBR', 'RBS', 'WRB'] #adverb, compartive, superlative, wh- adverb\n",
    "\n",
    "    tagcol = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    tags = [tagcol(word[0], word[1]) for word in tb.tags]\n",
    "\n",
    "    try:\n",
    "        pol = tb.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    try:\n",
    "        subj = tb.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    ncount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in nouns).values())\n",
    "    procount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in pronouns).values())\n",
    "    vcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in verbs).values())\n",
    "    adjcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adjectives).values())\n",
    "    advcount = sum(collections.Counter(tag.pos for tag in tags if tag.pos in adverbs).values())\n",
    "\n",
    "    # print('polarity {} subjectivity {}'.format(pol, subj))\n",
    "    # print('pos tags: {}'.format(posstring))\n",
    "    # trying = TextBlob(df['text'][0]).tags\n",
    "    # tag = collections.namedtuple('tag', ['word', 'pos'])\n",
    "    # tags = [tag(thing[0], thing[1]) for thing in trying]\n",
    "    return [pol, subj, ncount, procount, vcount, adjcount, advcount]\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# declare some strings\n",
    "id_column = 'id'\n",
    "missing_token = ' UNK '\n",
    "\n",
    "# read in our data, parse_dates=['column name'] will read that column as a datetime object, can take a boolean, list of integers / names, list of lists or a dictionary,\n",
    "# does different things depending on which one you use read the docs~\n",
    "train = pd.read_csv('data/train.csv', parse_dates=['project_submitted_datetime'])\n",
    "# test = pd.read_csv('data/test.csv', parse_dates=['project_submitted_datetime'])\n",
    "hopes = pd.read_csv('data/resources.csv').fillna(missing_token)\n",
    "\n",
    "# # lets make a master df of the train and test data to make our lives easier!\n",
    "# df = pd.concat([train,test], axis=0)\n",
    "# no lets not that was awful\n",
    "df = train\n",
    "\n",
    "# A new column for total price\n",
    "hopes['total_price'] = hopes['quantity']*hopes['price']\n",
    "\n",
    "# Make an aggregate df to join to our normal df\n",
    "# the .agg method takes in a function, string, or a dictionary or list of strings or functions.  The dictionary keys will be column names upon which functions should be run\n",
    "# I named it after the horse in Shadow of the Colossus~ the description column is now a count of how many, so it can be renamed to (number of )items\n",
    "agro = {'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}\n",
    "aggregatedf = hopes.groupby('id').agg(agro).rename(columns={'description':'items'})\n",
    "\n",
    "# now lets use that string functionality of .agg to get the min, max, and mean values!\n",
    "for maths in ['min', 'max', 'mean']:\n",
    "    # romanized Japanese horse name from game, and that guy that changes names in ff because why not lets have fun with variable names they're just for here anyway\n",
    "    aguro = {'quantity':maths, 'price':maths, 'total_price':maths}\n",
    "    namingway = {'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}\n",
    "    \n",
    "    # do some aggregation and join it to our previously created df\n",
    "    temporary = hopes.groupby('id').agg(aguro).rename(columns=namingway).fillna(0)\n",
    "    aggregatedf = aggregatedf.join(temporary)\n",
    "# This didn't work whoops # aggregatedf = aggregatedf.join([hopes.groupby('id').agg({'quantity':maths, 'price':maths, 'total_price':maths}).rename(columns={'quantity':maths+'_quantity', 'price':maths+'_price', 'total_price':maths+'_total_price'}).fillna(0) for maths in ['min', 'max', 'mean']])\n",
    "\n",
    "# and finally give it the original description columns aggregated together with a space in between them\n",
    "aggregatedf = aggregatedf.join(hopes.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n",
    "\n",
    "# Join that together with our everything df and check it out\n",
    "df = df.join(aggregatedf, on='id')\n",
    "df.head()\n",
    "\n",
    "# using datetime to make the above features\n",
    "df['year'] = df['project_submitted_datetime'].dt.year\n",
    "df['month'] = df['project_submitted_datetime'].dt.month\n",
    "df['year_day'] = df['project_submitted_datetime'].dt.dayofyear\n",
    "df['month_day'] = df['project_submitted_datetime'].dt.day\n",
    "df['week_day'] = df['project_submitted_datetime'].dt.weekday\n",
    "df['hour'] = df['project_submitted_datetime'].dt.hour\n",
    "\n",
    "# fill empty values with missing token ' UNK '\n",
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)\n",
    "\n",
    "# get length of each essay and its title\n",
    "df['essay1_len'] = df['project_essay_1'].apply(len)\n",
    "df['essay2_len'] = df['project_essay_2'].apply(len)\n",
    "df['essay3_len'] = df['project_essay_3'].apply(len)\n",
    "df['essay4_len'] = df['project_essay_4'].apply(len)\n",
    "df['title_len'] = df['project_title'].apply(len)\n",
    "\n",
    "# Combine the essays into one string\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']),\n",
    "                                            str(row['project_essay_2']),\n",
    "                                            str(row['project_essay_3']),\n",
    "                                            str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "# get our delicious features from that massive text\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count'] + 1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "\n",
    "df[['polarity', 'subjectivity', 'noun_count', 'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count']] = pd.DataFrame([(blob_the_text(row['text'])) for index, row in df.iterrows()], index = df.index)\n",
    "\n",
    "df.to_csv('../data/training_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF style features\n",
    "+ 1-3 NGram TF-IDF for Article Text at word level\n",
    "+ 1-3 NGram TF-IDF for Project Title at word level\n",
    "+ 1-3 NGram TF-IDF for Resource Text at word level\n",
    "+ 1-3 NGram TF-IDF for Article Text at character level\n",
    "+ 1-3 NGram TF-IDF for Project Title at character level\n",
    "+ 1-3 NGram TF-IDF for Resource Text at character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['teacher_number_of_previously_posted_projects', 'project_is_approved', 'items', 'quantity', 'price', 'total_price', 'min_quantity', 'min_price', 'min_total_price',\n",
    "          'max_quantity', 'max_price', 'max_total_price', 'year', 'month', 'year_day', 'month_day', 'week_day', 'hour', 'essay1_len', 'essay2_len', 'essay3_len', 'essay4_len',\n",
    "          'title_len', 'char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'stopword_count', 'polarity', 'subjectivity', 'noun_count',\n",
    "          'pronoun_count', 'verb_count', 'adjective_count', 'adverb_count', 'text', 'resource_description', 'project_resource_summary', 'project_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = train.drop('project_is_approved', axis=1)\n",
    "y = train['project_is_approved']\n",
    "\n",
    "X['resource_text'] = X.apply(lambda row: ' '.join([str(row['resource_description']), str(row['project_resource_summary'])]), axis=1)\n",
    "X = X.drop(['resource_description','project_resource_summary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "article_text = list(X_train['text'].values)\n",
    "title_text = list(X_train['project_title'].values)\n",
    "resource_text = list(X_train['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "article_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "article_vectorizer.fit(article_text)\n",
    "article_word_tfidf = article_vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "resource_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "resource_vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = resource_vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 294 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = X_train.drop(['text', 'project_title', 'resource_text'], axis=1)\n",
    "extra = sp.sparse.csr_matrix(X_train.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136560, 35)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136560, 2535)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trained = sp.sparse.hstack((article_word_tfidf, extra))\n",
    "X_trained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "article_text = list(X_test['text'].values)\n",
    "title_text = list(X_test['project_title'].values)\n",
    "resource_text = list(X_test['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "article_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "article_vectorizer.fit(article_text)\n",
    "article_word_tfidf = article_vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "resource_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "resource_vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = resource_vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = X_test.drop(['text', 'project_title', 'resource_text'], axis=1)\n",
    "extra = sp.sparse.csr_matrix(X_test.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45520, 2535)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tested = sp.sparse.hstack((article_word_tfidf, extra))\n",
    "X_tested.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1 Score: 0.8486379613356766\n",
      "C: 10 Score: 0.8486379613356766\n",
      "C: 100 Score: 0.8486599297012303\n",
      "C: 1000 Score: 0.8486379613356766\n",
      "C: 10000.0 Score: 0.848615992970123\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for C in [1, 10, 100, 1000, 1e4]:\n",
    "    logreg = LogisticRegression(C=C)\n",
    "    logreg.fit(X_trained, y_train)\n",
    "    y_pred_class = logreg.predict(X_tested)\n",
    "\n",
    "    print('C: {} Score: {}'.format(C, metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and once more for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "article_text = list(X['text'].values)\n",
    "title_text = list(X['project_title'].values)\n",
    "resource_text = list(X['resource_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for article text\n",
    "article_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "article_vectorizer.fit(article_text)\n",
    "article_word_tfidf = article_vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for titles\n",
    "vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_word_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf for resource text\n",
    "resource_vectorizer = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "resource_vectorizer.fit(resource_text)\n",
    "resource_word_tfidf = resource_vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 376 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = X.drop(['text', 'project_title', 'resource_text'], axis=1)\n",
    "extra = sp.sparse.csr_matrix(X.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_training_arc = sp.sparse.hstack((article_word_tfidf, extra))\n",
    "X_training_arc = sp.sparse.hstack((title_word_tfidf, extra))\n",
    "X_training_arc = sp.sparse.hstack((resource_word_tfidf, extra))\n",
    "X_training_arc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182080, 2535)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training_arc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\activations.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   3147\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3148\u001b[0m     \"\"\"\n\u001b[1;32m-> 3149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=5000, activation='relu', input_dim=2535))\n",
    "model.add(Dense(units=5000, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-730e00fdf425>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2535\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\activations.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\t0nb3\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   3147\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3148\u001b[0m     \"\"\"\n\u001b[1;32m-> 3149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a dictionary mapping tokens to their tfidf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['resource_word_tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['resource_word_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.96 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource_word_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>branches book</th>\n",
       "      <td>7.823436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diaries dork diaries</th>\n",
       "      <td>7.802456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superbright</th>\n",
       "      <td>7.792129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dork diaries dork</th>\n",
       "      <td>7.785303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diaries dork</th>\n",
       "      <td>7.781908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>branches</th>\n",
       "      <td>7.622444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12 amp 34</th>\n",
       "      <td>7.610950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34 18 34</th>\n",
       "      <td>7.610950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp 34 18</th>\n",
       "      <td>7.610950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper 12 amp</th>\n",
       "      <td>7.608097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34 18</th>\n",
       "      <td>7.605252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reeds</th>\n",
       "      <td>7.588350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leapfrog</th>\n",
       "      <td>7.582779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18 34</th>\n",
       "      <td>7.555380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lakeshore fully washable</th>\n",
       "      <td>7.518240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          resource_word_tfidf\n",
       "branches book                        7.823436\n",
       "diaries dork diaries                 7.802456\n",
       "superbright                          7.792129\n",
       "dork diaries dork                    7.785303\n",
       "diaries dork                         7.781908\n",
       "branches                             7.622444\n",
       "12 amp 34                            7.610950\n",
       "34 18 34                             7.610950\n",
       "amp 34 18                            7.610950\n",
       "paper 12 amp                         7.608097\n",
       "34 18                                7.605252\n",
       "reeds                                7.588350\n",
       "leapfrog                             7.582779\n",
       "18 34                                7.555380\n",
       "lakeshore fully washable             7.518240"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 15 highest tf-idf from that list\n",
    "tfidf.sort_values(by=['resource_word_tfidf'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Character level tf-idfs\n",
    "# article text\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(article_text)\n",
    "article_char_tfidf = vectorizer.transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# project title\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(title_text)\n",
    "title_char_tfidf = vectorizer.transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# resource text\n",
    "vectorizer = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32)\n",
    "vectorizer.fit(resource_text)\n",
    "resource_char_tfidf = vectorizer.transform(resource_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Be Continued...  My feeble attempts that weren't anywhere near all encompassing are below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athing = resource_df[resource_df['id'] == 'p069063']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athing_length = len(athing)\n",
    "for row in athing.itertuples():\n",
    "    print(round(row[3] * row[4], 2))\n",
    "athing_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumprice = []\n",
    "numbought = []\n",
    "avgprice = []\n",
    "\n",
    "for row in train_df.itertuples():\n",
    "    try:\n",
    "        df = resource_df[resource_df['id'] == row[1]]\n",
    "        df_length = len(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resource_scrape(idnum):\n",
    "    df = resource_df[resource_df['id'] == idnum]\n",
    "    try:\n",
    "        foo = [round(row[3] * row[4], 2) for row in df.itertuples()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['project_is_approved'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['teacher_number_of_previously_posted_projects'].value_counts() > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
